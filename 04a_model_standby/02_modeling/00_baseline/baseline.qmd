---
+---
title: "Baseline Modelling"
author: "Georg Grunsky"
format: html
editor: visual
bibliography: references.bib
---

```{r}
#| label: load packages
#| message: false
#| warning: false
#| include: false

library(fable)
library(tsibble)
library(lubridate)
library(dplyr)
library(tidyverse)
library(readr)
library(plotly)
library(forecast)
library(feasts)
require(fpp3)
require(ggplot2)
```

## Prämisse

### Use Case Analyse {#sec-use-case-analyse}

In der vorausgegangenen Use Case Analyse [@grunsky_rettungsdienst_2024] wurden bereits Überlegungen zu Modellansätzen sowie der Bewertung dieser angestellt. Diese wurden, vor allem, in den Abschnitten **Building Models** und **Impact Simulation** behandelt. Die Erkenntnisse werden nachfolgend nochmals zusammengefasst:

In der Use Case Analyse wurden die überblicksmäßig die Stärken und Schwächen verschiedener Machine Learning Algorithmen verglichen. Aufgrund der Einfachheit, Interpretierbarkeit und Tauglichkeit für saisonale Zeitreihen mit wenig Features schienen die Modelle SARIMA und Facebook Prophet den Anforderungen dieses Anwendungsfalles entgegenzukommen.

**SARIMA** (Seasonal Autoregressive Integrated Moving Average) stellt dabei ein klassisches Modell für saisonale Zeitreihenanalysen dar. **Facebook Prophet** wurde ebenso für Zeitreihenvorhersagen entwickelt und verwendet dabei ein zusammengesetztes Modell aus Trend, Saisonalität und speziellen Ereignissen in Zeitreihen. [@taylor_forecasting_2017].

Eine weitere, sehr "schlanke", aber nicht so fein skalierbare Methode für die Vorhersage saisonaler Daten stellt **SNAIVE** (Seasonal Naive Model) dar. Dieser Algorithmus .... eignet sich daher für ein Baseline Model ...

Auf Basis des DRK-Reformtarifvertrags [@deutsches_rotes_kreuz_drk-reformtarifvertrag_2023] wurde geschätzt, dass ein Tag mit, bisher fix eingeteilten, 90 Bereitschaftsfahrer:innen etwa €14.850,- kostet (€165,- pro Bereitschaftsfahrer:in), die an vielen Tagen gar nicht abgerufen werden. Diese Kosten können gut als Messwerte in eine Kostenfunktion für die Güte des Modells integriert werden. Ein deutlich höherer Strafbetrag für eine zu niedrige Schätzung soll den Anforderungen des K.O.-Kriteriums entsprechen, nie zu wenig Bereitschaftspersonal vorzusehen.

### Explorative Datenanalyse

Die Erkenntnisse der explorativen Datenanalyse bieten eine weitere Basis für die Erstellung eines Baseline Models und lassen sich wie folgt zusammenfassen:

1.  Beim Bedarf an StandBy-Personal wurde über die Jahre ein positiver Trend mit einem starken Anstieg seit Beginn des Jahres 2019 beobachtet.
2.  Ebenso wurde eine
    1.  starke wochenweise Saisonalität mit Perioden zw. 3-5 Wochen,
    2.  eine Abhängigkeit zum Tag im Monat mit einem starken Abwärtstrend zu Beginn jeden Monats und Periodenlängen von etwa drei Tagen,
    3.  sowie eine schwächer ausgeprägte monatliche Saisonalität festgestellt.
3.  Es wurde außerdem festgehalten, dass eine Abhängigkeit des Bedarfs an StandBy-Personal von der Anzahl krankgemeldeter Bereitschaftsfahrer:innen und der Anzahl eingehender Notrufe plausibel scheint. In der wochenweisen Aggregation der Daten bestätigte das neu entstandene Merkmal call_per_sick (eine Division der Anzahl der Notrufe durch die Anzahl der Krankmeldungen) diese Annahme. Dies wurde vor allem in der logarithmischen Darstellung der Daten ersichtlich.

Die möglichen Prediktoren aus dem letzten Punkt müssten für eine Verwendung als solche selbst erst einzeln vorhergesagt und danach zusammengeführt werden. Ein direktes Lernen der zeitlichen Muster der abhängigen Variable scheint an dieser Stelle für ein Baseline Model effizienter. Die ersten beiden Punkte weisen auf eine Kombination aus Trends und unterschiedlichen Saisonalitäten hin, die in einer Vorhersage berücksichtigt werden muss. Die Verwendung von dem oben beschriebenen Facebook Prophet Alorithmus für ein Baseline Model scheint den Anforderungen zu entsprechen.

## Baseline Model mit SNAIVE

SNAIVE und fable

### Kostenfunktion

Wie bereits in @sec-use-case-analyse beschrieben, nimmt die unten angeführte Kostenfunktion Bezug auf entstandene (jedoch nicht abgerufene) StandBy-Kosten, sowie Strafkosten für zu niedrige Vorhersagen. Die Werte werden zum Vergleich über die Testperiode aufsummiert.

```{r}
#| eval: true
#| echo: true
#| message: false
#| warning: false
#| include: false

lossfunction <- function(obs, pred, type = sum) {
  
  resid <- obs - pred
  assign("method",type)
  
  # Kosten pro zu hoch geschätzter Bereitschaft in Höhe von €165,- 
  stdbyloss <- get("method")(pmax(-resid,0)) * 165
  
  # Deutlich höherer Strafbetrag für zu niedrige Schätzung (unabhängig der Anzahl). 
  ifelse(get("method")(pmax(resid,0)) > 0, 
         penalty <- 1000000,
         penalty <- 0)
  
  # Summenbildung für Gesamtkosten
  loss <- stdbyloss + penalty
  
  return(loss)
}


# Für den Vergleich von Kosten über einen längeren Zeitraum
# kann mitdieser Funktion die Lesbarkeit erhöht werden.

errorcalc <- function(type, nums) {
  
  assign("method",type)
  round(get("method")(nums)/1000,0)*1000
  
}
```

### Pre-Processing {#sec-pre-processing}

#### Reading the csv-file

```{r}
#| eval: true
#| echo: true
#| message: false
#| warning: false
#| include: false

sickness_csv <- read_csv(paste0(wd,"/01_data/01_lake/01_use_case_2/sickness_table.csv"), 
    col_types = cols(date = col_datetime(format = "%Y-%m-%d")))

sickness_csv$date <- as.Date(sickness_csv$date)
sickness_csv$value  <- log(sickness_csv$sby_need + 1)

# tageweisen tsibble erstellen mit logarithmischen Daten 
data <- sickness_csv %>% 
  select(date, value) %>%
  as_tsibble(index = date)

# wochenweise nach "max" aggregierten tsibble erstellen
ts_week <- sickness_csv %>%
  mutate(week = yearweek(date)) %>%
  select(week, sby_need) %>%
  reshape2::melt(id.vars = "week") %>%
  group_by(week) %>%
  summarise(max_value = max(value)) %>%
  mutate(log_value = log(max_value + 1)) %>%
  as_tsibble(index = week)
```

Im bisherigen Bereitschaftssystem wurden täglich fix 90 StandBy-Fahrer:innen vorgehalten. Nach der obigen Kostfunktion wären hierdurch, für den, im Code angeführten, Testzeitraum Kosten in der Höhe der Codeausgabe entstanden. Diese können zu einem späteren Zeitpunkt einen Vergleichswert liefern.

```{r}

# Testzeitraum
teststart <- "2019-02-01"
testend <- "2019-05-31"

# Kostenfunktion über Testzeitraum mit aufsummierten Kosten
lossfunction(obs = data[between(data$date,
                                as.Date(teststart),
                                as.Date(testend)),]$value,
             pred = log(90),
             type = sum)



```

### Data Splitting

Data Splitting / Kreuzvalidierung in Zeitreihen blabla wie funktionierts

```{r}
# Kreuzvalidierung
cv <- data %>%
  slide_tsibble(.size = 360, .step = 60)
```

### Model Training

#### fable

```{r}

# Training
system.time(
  progressr::with_progress(
    model <- cv %>%
      model(snaive = SNAIVE(value),
            naive = NAIVE(value),
            mean = MEAN(value),
            drift = RW(value),
            ma = ARIMA(value ~ pdq(0, 0, 2)),
            ets_season = ETS(value ~ error("A") + trend("A") + season("A")),
            ets = ETS(value)) 
    )
)

```

### Prediction

```{r}

# Vorhersage
fc <- model %>%
  forecast(h = "60 days" )

```

### Measuring Performance

```{r}


# setzen des zu kalkulierenden Konfidenzlevels
lev = 95

# zusammenbauen eines auswertbaren Datensatzes
eval <- data.frame(id = fc$.id,
                   model = fc$.model,
                   date = fc$date,
                   dist = fc$value, # Wahrscheinlichkeitsverteilung
                   predicted = fc$.mean) %>%
  filter(date <= as.Date("2019-05-27"))

# Erweiterung um die Schätzungen der Konfidenzintervalle der Wahrscheinlichkeitsverteilungen
eval <- eval %>% rowwise %>% 
  mutate(lower = pmax(unlist(distributional::hdr(dist,lev)),0)[1],
         upper = pmax(unlist(distributional::hdr(dist,lev)),0)[2],
         level = pmax(unlist(distributional::hdr(dist,lev)),0)[3])

# Anwendung der Kostenfunktion auf den Punktschätzer und die Obergrenze des Konfidenzintervalles für jeden Schätzwert
singlecosts <- eval %>% 
  select(model, id, date, predicted, lower, upper) %>%
  left_join(data, by = "date") %>%
  mutate(dailycost = lossfunction(obs = exp(value), pred = exp(predicted)),
         dailyupper = lossfunction(obs = exp(value), pred = exp(upper)))

```

eigentlich geht nur ein mittelmaß wegen der überschneidenden zeitbereiche oder verwendung mehrerer models, deswegen wird wo nötig max() verwendet.

Um die Kostenfunktion anzuwenden müssen die Vorhersagen auf den Zeitraum der vorhandenen Vergleichsdaten gekürzt werden.

#### Vergleich der Modelle

```{r}


compare_models <- singlecosts %>% group_by(model) %>%
  summarise(maxerror = errorcalc(max, na.omit(dailycost)),
            meanerror = errorcalc(mean, na.omit(dailycost)),
            medianerror = errorcalc(median,na.omit(dailycost)),
            # maxupper = errorcalc(max,na.omit(dailyupper)),
            # meanupper = errorcalc(mean,na.omit(dailyupper)),
            # medianupper = errorcalc(median,na.omit(dailyupper)),
            n_high = sum(na.omit(
              dailycost)[na.omit(dailycost) == 1000000])/1000000)

compare_models
```

## **1. `autoplot()` – Funktion und Interpretation**

```{r}

# Visualisierung von Modell + Vorhersage
p <- autoplot(data) +
  autolayer(fc, level = c(80,95)) +
  labs(title = "Prognoseplot",
       y = "Wert",
       x = "Datum")

ggplotly(p)
```

```{r}
# Kreuzvalidierung
cv <- mq %>%
  slide_tsibble(.size = 180, .step = 60)

# Training
system.time(
  progressr::with_progress(
    model <- cv %>%
      model(#snaive = SNAIVE(value),
            arima = ARIMA(value ~ pdq())
            # ets_season = ETS(value ~ error("N") + trend("N") + season("A")),
            #ets = ETS(value)
            ) 
    )
)


# Vorhersage
fc <- model %>%
  forecast(h = "60 days" )
```

```{r}
# Visualisierung von Modell + Vorhersage
p <- autoplot(data) +
  # autolayer(mq) + 
  autolayer(fc, level = c(80,95)) +
  labs(title = "Prognoseplot",
       y = "Wert",
       x = "Datum")

ggplotly(p)
```

```{r}
# Kreuzvalidierung
cv <- ts_week %>%
  stretch_tsibble(.init = 106, .step = 3)

decomp_spec <- decomposition_model(
  # STL(log(max_value + 1)),
  STL(log(max_value + 1) ~ trend(window = 53) +
                   season(period = "1 year")),
  ETS(season_adjust ~ season("N"))
)

# Training
system.time(
  progressr::with_progress(
    model <- cv %>%
      model(
        stl_ets = decomp_spec
      )
  )
)


# Vorhersage
fc <- model %>%
  forecast(h = "10 weeks", level = c(60,95)) 

ts_week %>%
  autoplot(max_value, colour = "red") +
  autolayer(fc, alpha = 0.3) +
  labs(y = "log-Werte an Personalbedarf",
       title = "Wochenmaximum an Bereitschaftspersonal")
```

```{r}
fc %>%
  accuracy(
    list(qs = quantile_score), probs = 0.10
    )
```

```{r}
cv <- ts_sick_mq %>%
  stretch_tsibble(.init = 106, .step = 3)

decomp_spec <- decomposition_model(
  STL(mq2 ~ trend(window = 53) +
                   season(period = "1 year")),
  ETS(season_adjust ~ season("N"))
)

# Training
system.time(
  progressr::with_progress(
    model <- cv %>%
      model(
        stl_ets = decomp_spec,
        snaive = SNAIVE(mq2),
        arima = ARIMA(mq2 ~ pdq()),
        ets_season = ETS(mq2 ~ error("N") + trend("N") + season("A")),
        ets = ETS(mq2)
        )
  )
)


# Vorhersage
system.time(
  progressr::with_progress(
    fc <- model %>%
      forecast(h = "10 weeks", level = c(95))
  )
)



p <- ts_sick_mq %>%
  filter(date >= "2019-04-01") %>%
  autoplot(sby_need, colour = "red") +
  autolayer(filter(fc,date >= "2019-04-01"),
            alpha = 0.3) +
  labs(y = "Personalbedarf",
       title = "Bedarf an Bereitschaftspersonal")

ggplotly(p)
```

\

```{r}
fc %>%
  accuracy(
    list(qs = quantile_score), probs = 0.10
    )
```
