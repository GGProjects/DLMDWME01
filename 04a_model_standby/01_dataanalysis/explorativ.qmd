---
title: "Exploratory Analysis: StandyBy Data"
author: "Georg Grunsky"
editor: visual
bibliography: references.bib
---

Dieses Dokument beschreibt die vorbereitenden Abschnitte Prüfung der Datenqualität und Explorative Datenanalyse für den bestehenden Use Case. Die Bereiche Datenaufbereitung und Modellierung werden in den jeweiligen Modellverzeichnissen behandelt. [@pak_fallstudie_2022]

# Feststellen der Datenqualität

## Datensätze einlesen

Die Daten werden in zwei Dateien bereitgestellt: ***sickness_table.csv*** und ***sickness_table.xlsx***. Augenscheinlich handelt es sich dabei um den selben Datensatz,was jedoch eingangs noch zu überprüfen ist.

```{r}
#| echo: true
#| message: false
#| warning: false

require(readr)
path_to_samples <- "../00_sample_data/"
sickness_csv <- read_csv(paste0(path_to_samples,"01_raw/sickness_table.csv"), 
    col_types = cols(date = col_datetime(format = "%Y-%m-%d")))
head(sickness_csv)
```

```{r}
#| echo: true
#| message: false
#| warning: false

require(readxl)
sickness_xls <- read_excel(paste0(path_to_samples,"01_raw/sickness_table.xlsx"), 
    col_types = c("numeric", "date", "numeric", 
        "numeric", "numeric", "numeric", "numeric", 
        "numeric"))
head(sickness_xls)
```

## Datensätze vergleichen

```{r}
#| echo: true
#| message: false
#| warning: false

identical(sickness_csv, sickness_xls)
```

Die Überprüfung mittels *identical* besagt, dass die Datensätze nich identisch sind. Die in den jeweiligen Daten-Dictionaries beschriebenen Datenstrukturen lassen jedoch schon darauf schließen. Eine Summary der Wertebereiche der vorliegenden Daten schafft mehr Klarheit.

## Daten Summary

### sickness_csv

```{r}
#| echo: true
#| message: false
#| warning: false

summary(sickness_csv)
```

### sickness_xls

```{r}
#| echo: true
#| message: false
#| warning: false

summary(sickness_xls)
```

Gemäß den angezeigten Wertebereichen handelt es sich tatsächlich um die gleichen Datensätze.

Anhand der .csv Datei wird noch überprüft ob der Datensatz fehlende Werte beinhaltet, bevor mit der weiteren Datenexploration fortgefahren wird.

## NAs und "Missing Values"

```{r}
#| echo: true
#| message: false
#| warning: false

alldays <- seq(from = as.Date("2016-04-01"),
               to = as.Date("2019-05-27"),
               by = 1)

if (identical(alldays, as.Date(sickness_csv$date))) print("no missing dates") else print("you have to deal w/ missing dates")

if (!anyNA.data.frame(sickness_csv)) print("no missing values") else print("you have to deal w/ missing values")
```

In dieser Zeitreihe haben wir weder mit fehlenden Zeitwerten noch NAs in den Datenpunkten umzugehen. Die Prüfung der Datenqualität wird somit abgeschlossen.

## Zusammenfassung

1.  Die bereitgestellten Datensätze scheinen identisch zu sein. Es wird nur mit der .csv Datei weitergearbeitet.

```{r}
#| message: false
#| warning: false
#| include: false

rm(sickness_xls,
   path_to_samples,
   alldays)
```

2.  Es gibt keine fehlenden Zeitwerte und/oder NAs in den Datenpunkten
3.  Der bisherige Fixwert von 90 Bereitschaftspersonen (*n_sby*) scheint in den meisten Fällen im Vergleich zu *sby_need* zu hoch zu sein. Es gibt jedoch auf Tage wo mehr als 90 Personen gebraucht werden.
4.  Die Zeitreihe behandelt den Zeitraum von 2016-04-01 bis 2019-05-27. Eine Periode von drei Jahren vor der COVID Pandemie. Es ist davon auszugehen, dass COVID die aktuellen Zahlen verändert hat. Denoch hilft eine erfolgreiche Umsetzung einer Vorhersage auf den Testdaten, das Projekt mit aktuellen Daten voranzutreiben und zukünftig Bereitschaftskosten sparen zu können.
5.  Die Anzahl der Anrufe und die Anzahl des krankgemeldeten Personals scheint ähnliche Proportionen aufzuweisen, nur um etwa den Faktor 100 kleiner.

# Explorative Datenanalyse

@hyndman_forecasting_2021 bietet von der Exploration bis zur Modellierung einen sehr ausführlichen Anhalt für die Arbeit mit Zeitreihendaten. Ein weiteres R-Package, das gute Werkzeuge für die explorative Analyse aber auch für die Erstellung von Models anbietet ist *caret*. Das Paket eignet sich zwar weniger gut für Zeitreihendaten, aus der Dokumentation alleine kann aber bereits viel gelernt werden. [@kuhn_caret_2019].

## Merkmalsübersicht

Datenplots unterstützen in der Feststellung von Abhängigkeiten und Verteilungen. Die erste Spalte (*id*), die bisher konstante Einteilung des Bereitschaftspersonals (*n_sby*), sowie die Anzahl der zusätzlich benötigten Fahrer:innen (*dafted*) werden, zur besseren Übersicht, hierbei bereits ausgeblendet. Da es sich um Zeitreihendaten handelt, wird der Datensatz auch in ein entsprechendes Objekt konvertiert.

```{r}
#| echo: true
#| message: false
#| warning: false

require(dplyr)
require(fpp3)

ts_sby <- sickness_csv %>%
  select(-c("...1", "n_sby", "dafted")) %>%
  mutate(date = as.Date(date)) %>%
  as_tsibble(index = date)

ts_sby %>%
  GGally::ggpairs()
```

*sby_need* (die Zielvariable) korreliert augenscheinlich am meisten mit der Anzahl der Notfallanrufe (*calls*). Das klingt durchaus plausibel. Die vermutete Saisonalität ist in der Übersicht der Variablen *sby_need*, *calls* und *n_sick* (der Anzahl der krankgemeldeten Bereitschaftsfahrer:innen), mit dem menschlichen Auge, gut zu erkennen. und wirkt bei dem Merkmal *calls* am Stabilsten.

Das Merkmal *n_duty* (die Anzahl der diensthabenden Bereitschaftsfahrer:innen) weist nur drei unterschiedliche Werte auf, die ausschließlich eine Abhängigkeit zum Datum haben. Hierbei wurde die Anzahl der Diensthabenden jedes Jahr zum ersten Januar um 100 Personen erhöht. Am 01.01.2019 ist dies jedoch nicht geschehen. Dieser Umstand muss vermutlich gesondert mit den Entscheidungsträgern besprochen und ggf. nachgezogen werden.

*n_sick* zeigt zwar die angesprochene Saisonalität, weist, aufgrund der o.a. Grafik, aber nur eine geringe Korrelation zur Zielvariablen auf. Es scheint plausibel, dass eine höhere Anzahl an Krankenständen, bei gleichbleibenden oder steigenden Notfällen zu einem höheren Bedarf an Bereitschaftspersonal führt. Möglicherweise unterstützt die Kombination dieser beiden Merkmale eine erfolgsversprechende Vorhersage.

Für die Vorhersage des Merkmals *sby_need* gibt es zu diesem Zeitpunkt daher fünf mögliche Ansatzpunkte.

1.  Die direkte Vorhersage aufgrund des Datum und der eigenen Saisonalität
2.  Eine indirekte Vorhersage aufgrund der Saisonalität des Merkmals *calls* und der, ab einem gewissen Wert, nahezu linear anzunehmenden Korrelation mit *sby_need*. Letztere zeigt jedoch "drei Liniaritäten" und ist wahrscheinlich durch *n_duty* beeinflusst.
3.  Wie in Punkt 2., nur dass zusätzlich eine jährliche Steigerung von *n_duty* berücksichtigt wird um aufgetretene Trends abzuflachen und mehr Fokus auf Saisonalität legen zu können. Dieses zu generierende Merkmal wird als regulierte calls, *reg_calls* bezeichnet.
4.  Ein indirekte Vorhersage aufgrund der Saisonaltität eines neuen kombinierten Merkmals aus *calls* und *n_sick*, die jedoch zuerst zu evaluieren ist. Das neue Merkmal wird *calls_sick* genannt.
5.  Wie in Punkt 4., aber wiederum unter Berücksichtigung einer jährlichen Steigerung von *n_duty* um aufgetretene Trends abzuflachen. Als Bezeichnung wird *reg_calls_sick* verwendet.

Die weitere Analyse konzentriert sich daher auf die ursprünglichen Merkmale *date*, *sby_need* und *calls* sowie auf die neu generierten Variablen.

Die in Punkt 4. beschriebene kombinierte Variable *calls_sick* aus *calls* und *n_sick* wird als Anrufe je krankgemeldetem/r Einsatzfahr:in verstanden und daher berechnet als

$$
calls\_sick_t =\frac{calls_t}{n\_sick_t}
$$

Die Werte von *n_duty* werden für die Analyse ab 01.01.2019 korrigiert und für die Merkmalsgenerierung "regulierter Calls" (*reg_calls*) und *reg_calls_sick* verwendet.

$$
reg\_calls_t = calls_t - n\_duty_t\ \ |\ \   n\_duty_t = 1700 + (year(t)-2016)*100
$$ $$
reg\_calls\_sick_t = \frac{reg\_calls_t}{n\_sick_t}
$$

```{r}
#| echo: true
#| message: false
#| warning: false

ts_sby <- ts_sby %>%
  mutate(reg_calls = calls - (1700 + (year(date)-2016) * 100),
         calls_sick = calls/n_sick,
         reg_calls_sick = reg_calls/n_sick) %>%
  select(-c("n_duty", "n_sick"))

ts_sby %>%
  GGally::ggpairs()

```

Um festzustellen welcher Ansatz möglicherweise erfolgversprechender ist, lohnt es sich die Variablen auf die vermeintliche Vorhersagbarkeit ihrer Saisonalität hin zu überprüfen. @hyndman_forecasting_2021 verweist hierbei auf die Spektrale Entropy (Shannon) einer Zeitreihe, die einen Wert zwischen 0 und 1 ausgibt. Je niedriger der Wert, desto stärker ist die Saisonalität und der Trend der Zeitreihe.

```{r}
#| echo: true
#| message: false
#| warning: false

ts_sby %>%
features_all(feat_spectral)
```

Interessanterweise, ist trotz einer augenscheinlich gut erkennbaren Saisonalität der Entropie-Wert durchgehend relativ hoch. Am ehesten scheint sich an dieser Stelle das unregulierte Merkmal der Notfallanrufe (*calls*) für eine saisonal-bedingte Vorhersage zu eignen. Dieses Merkmal scheint annähernd normalverteilt zu sein und die Korrelation zur Zielvariablen *sby_need* liegt nur leicht unter dem regulierten Wert *reg_calls*. *calls_sick* und *reg_calls_sick* schneiden bei der Betrachtung der letzten beiden Faktoren deutlich schlechter ab.

Der Autokorrelationsplot der Variablen *calls* soll weiteren Aufschluss über dieses Merkmal geben.

```{r}
#| echo: true
#| message: false
#| warning: false

ts_sby %>% 
  ACF(y = calls, lag_max = 90) %>%
  autoplot()
```

Die höhere Autokorrelation in den niedrigen "lags", die im Verlauf abnimmt, zeigt, dass die Daten einem Trend unterliegen. Die "kleinen" Peaks bei 7, 14, 21, etc. zeigen eine leichte wochenweise Saisonalität, der etwas höhere bei 28 könnte auch auf eine monatliche Saisonalität hindeuten.

## Decomposition

```{r}
ts_sby %>%
  model(
    STL(calls ~ trend(window = 121) +
          season(period = "1 month", window = 61))) %>%
  components() %>%
  autoplot()
```

Interpretation

# Verteilung relevanter Merkmale

Die Verteilung der Zielvariablen und des möglichen Prädiktors soll schlussendlich noch Auschluss, bzw. einen Überblick über die zu erwartenden Werte liefern.

```{r}
ggplot() + 
  geom_density(data = ts_sby, aes(x = log(sby_need+1), fill = "red")) +
  geom_density(data = ts_sby, aes(x = log(calls), fill = "blue"))
```

erkannt: Masse bei den Werte 0-10

kleine Gruppe an Werten von 418 bis 456 aus jedem Jahr =\> selber Tag? jährliche Veranstaltung?

```{r}
table(year(ts_sby[between(ts_sby$logscale,4,6),]$date))
range(ts_sby$date)
```

nicht bewahrheitet

? Quantillen als mögliche Anhaltepunkte für einen fixen "Sicherheitspolster" in der Vorhersage

```{r}

dens <- ggplot(data = ts_sby, 
               aes(x = sby_need, fill = as.factor(year(date)))) +
  geom_density()
ggplotly(dens)

summary(ts_sby$sby_need)

sum(table(ts_sby[ts_sby$sby_need <= 10,]$sby_need))/1152


quantile(ts_sby$sby_need, probs = c(0.95, 0.90, 0.85, 0.8, 0.68))
```

Quantillen 68% und 75% 80% und 85%, wenige aber starke Ausreißer =\> logarithmische Skalierung um die Werte besser zu verteilen.

```{r}

ts_sby$logscale <- log(ts_sby$sby_need + 1)

dens <- ggplot(data = ts_sby, 
               aes(x = logscale, fill = as.factor(year(date)))) +
  geom_density()
ggplotly(dens)

summary(ts_sby$logscale)

quantile(ts_sby$logscale, probs = c(0.95, 0.90, 0.85, 0.8, 0.75, 0.68))


```

besser erkennbar in Grafik und "schöner" verteilt. Beschreibung der Grafik

die Jahre 2016 und 2017 sind nahezu gleich, Änderungen vor allem in 2018 und im ersten Drittel 2019. Dennoch in allen Jahren eine mehr oder weniger ausgprägte "zweite Erhebung" zw. den Werten 4 und 6 erkennbar.

Selbst wenn 50 als Fixwert mit Sicherheitsreserve (statt 0-Vorhersagen) gewählt wird, können täglich immer noch (90 - 50) \* 165 Euro (€6.600,-), nur anhand der explorativen Datenanalyse, eingespart werden und man hätte mit 80% Wahrscheinlichkeit immer noch genug StandBy-Personal. Wenn der Vorhersage-Algorithmus die restlichen 20% Ausreißer "findet", ist das Ziel erreicht.

Algorithmen werden vermutlich mit log-Werten besser umgehen können, weil sich Ausreißer weniger Stark auswirken.

```{r}
p <- ts_sby %>%
  mutate(
    MA8 = slider::slide_dbl(calls, mean,
                            .before = 3, .after = 4, .complete = TRUE),
    MA2_8 = slider::slide_dbl(MA8, mean,
                                  .before = 1, .after = 0, .complete = TRUE)
        ) %>%
     #filter(date > "2018-01-01") %>%
  gg_season(MA2_8, period = "1y")

ggplotly(p)
```

am moving average von calls gut erkennbar, 2 saisonalitäten monatlich und jährlich, starker trend vor allem zu beginn 2019
```{r}
ts_sby %>%
    mutate(
    MA8 = slider::slide_dbl(calls, mean,
                            .before = 3, .after = 4, .complete = TRUE),
    MA2_8 = slider::slide_dbl(MA8, mean,
                                  .before = 1, .after = 0, .complete = TRUE)
        ) %>%
features_all(feat_spectral)
```



# Fazit

# Speichern aufbereiteter Daten



# Literaturverzeichnis
