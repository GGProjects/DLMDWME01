---
title: "Exploratory Analysis: StandyBy Data"
author: "Georg Grunsky"
editor: visual
bibliography: references.bib
---

Dieses Dokument beschreibt die vorbereitenden Abschnitte Prüfung der Datenqualität und Explorative Datenanalyse für den bestehenden Use Case. Die Bereiche Datenaufbereitung und Modellierung werden in den jeweiligen Modellverzeichnissen behandelt. [@pak_fallstudie_2022]

# Feststellen der Datenqualität

## Datensätze einlesen

Die Daten werden in zwei Dateien bereitgestellt: ***sickness_table.csv*** und ***sickness_table.xlsx***. Augenscheinlich handelt es sich dabei um den selben Datensatz,was jedoch eingangs noch zu überprüfen ist.

```{r}
#| echo: true
#| message: false
#| warning: false

require(readr)
path_to_samples <- "../00_sample_data/"
sickness_csv <- read_csv(paste0(path_to_samples,"01_raw/sickness_table.csv"), 
    col_types = cols(date = col_datetime(format = "%Y-%m-%d")))
head(sickness_csv)
```

```{r}
#| echo: true
#| message: false
#| warning: false

require(readxl)
sickness_xls <- read_excel(paste0(path_to_samples,"01_raw/sickness_table.xlsx"), 
    col_types = c("numeric", "date", "numeric", 
        "numeric", "numeric", "numeric", "numeric", 
        "numeric"))
head(sickness_xls)
```

## Datensätze vergleichen

```{r}
identical(sickness_csv, sickness_xls)
```

Die Überprüfung mittels *identical* besagt, dass die Datensätze nich identisch sind. Die in den jeweiligen Daten-Dictionaries beschriebenen Datenstrukturen lassen jedoch schon darauf schließen. Eine Summary der Wertebereiche der vorliegenden Daten schafft mehr Klarheit.

## Daten Summary

### sickness_csv

```{r}
#| echo: true
#| message: false
#| warning: false

summary(sickness_csv)
```

### sickness_xls

```{r}
#| echo: true
#| message: false
#| warning: false

summary(sickness_xls)
```

Gemäß den angezeigten Wertebereichen handelt es sich tatsächlich um die gleichen Datensätze.

Anhand der .csv Datei wird noch überprüft ob der Datensatz fehlende Werte beinhaltet, bevor mit der weiteren Datenexploration fortgefahren wird.

## NAs und "Missing Values"

```{r}
#| echo: true
#| message: false
#| warning: false

alldays <- seq(from = as.Date("2016-04-01"),
               to = as.Date("2019-05-27"),
               by = 1)

if (identical(alldays, as.Date(sickness_csv$date))) print("no missing dates") else print("you have to deal w/ missing dates")

if (!anyNA.data.frame(sickness_csv)) print("no missing values") else print("you have to deal w/ missing values")
```

In dieser Zeitreihe haben wir weder mit fehlenden Zeitwerten noch NAs in den Datenpunkten umzugehen. Die Prüfung der Datenqualität wird somit abgeschlossen.

## Zusammenfassung

1.  Die bereitgestellten Datensätze scheinen identisch zu sein. Es wird nur mit der .csv Datei weitergearbeitet.

```{r}
#| message: false
#| warning: false
#| include: false

rm(sickness_xls)
```

2.  Es gibt keine fehlenden Zeitwerte und/oder NAs in den Datenpunkten
3.  Der bisherige Fixwert von 90 Bereitschaftspersonen (*n_sby*) scheint in den meisten Fällen im Vergleich zu *sby_need* zu hoch zu sein. Es gibt jedoch auf Tage wo mehr als 90 Personen gebraucht werden.
4.  Die Zeitreihe behandelt den Zeitraum von 2016-04-01 bis 2019-05-27. Eine Periode von drei Jahren vor der COVID Pandemie. Es ist davon auszugehen, dass COVID die aktuellen Zahlen verändert hat. Denoch hilft eine erfolgreiche Umsetzung einer Vorhersage auf den Testdaten, das Projekt mit aktuellen Daten voranzutreiben und zukünftig Bereitschaftskosten sparen zu können.
5.  Die Anzahl der Anrufe und die Anzahl des krankgemeldeten Personals scheint ähnliche Proportionen aufzuweisen, nur um etwa den Faktor 100 kleiner.

# Explorative Datenanalyse

@hyndman_forecasting_2021 bietet von der Exploration bis zur Modellierung einen sehr ausführlichen Anhalt für die Arbeit mit Zeitreihendaten. Ein weiteres R-Package, das gute Werkzeuge für die explorative Analyse aber auch für die Erstellung von Models anbietet ist *caret*. Das Paket eignet sich zwar weniger gut für Zeitreihendaten, aus der Dokumentation alleine kann aber bereits viel gelernt werden. [@kuhn_caret_2019].

## Mekrmalsübersicht

Datenplots unterstützen in der Feststellung von Abhängigkeiten und Verteilungen. Die erste Spalte (*id*), die bisher konstante Einteilung des Bereitschaftspersonals (*n_sby*), sowie die Anzahl der zusätzlich benötigten Fahrer:innen (*dafted*) werden, zur besseren Übersicht, hierbei bereits ausgeblendet.

```{r}
#| echo: true
#| message: false
#| warning: false

require(dplyr)

sickness <- sickness_csv %>%
  select(-c("...1", "n_sby", "dafted"))
plot(sickness)
```

Conclusion:

sby_need (the predicted variable) is apparently mostly depended on the number of calls, (which makes sense) and on dafted personnel (which will be ignored because that is to be avoided). A hint of dependency is also recognizable between the variables sby_need, calls and n_sick and the variable date, which could acknowledge the alleged seasonality of the data. It is advisable to shift the focus to these features.

Logically there should be a dependency between the number of sick personnel and the n_duty feature, which can't be seen in this plots. In the next section, we will add another plot with an ordered nb_sick column in comparison to n_duty and the number of emergency calls.

## Focus on specific variables {#sec-focus-on-specific-variables}

```{r}
#| message: false
#| warning: false
plot(sickness_csv[order(sickness_csv$n_sick),c(3:5)])
plot(sickness_csv[,c(2:4,7)])
```

The first plot shows, that n_duty has only three different values which are not affected by n_sick. We won't have influence on n_duty with our prediction model in the first run, so we can ignore that variable. It also shows that there seems to be no dependency between n_sick and calls. Still there are two ideas

1.  We know from a previous analysis that n_sick and calls possibly have a similar value distribution (only by a factor of 100) but are not related to each other. Still it will be difficult to get these two variables on the same plot. Let's try to add a feature with divided calls by 100 to get them on the same scale.
2.  It sounds logical, that the more sick personel the more reduced the ability to respond to emergency calls (even that n_duty was not related to n_sick). Probably it might be interesting to add another feature that shows calls per sick personnel to describe the mentioned ability to respond to emergencies. If there was a relation between those to, the average result would be around 100 (the factor), but fortunately there wasn't.

In the second figure, it seems even more clearly that the date has an influence on the other three variables. Let's therefore generate some more features out of the date variable to see if it is possible to be more specific about this fact. The shown figures analyse the three variables in comparison to the new features.

```{r}
#| message: false
#| warning: false
require(lubridate, quietly = T)
require(dbplyr, quietly = T)
sick <- sickness_csv[,c(2:4,7)]
sick <- sick %>% dplyr::mutate(year = lubridate::year(date),
                        month = lubridate::month(date),
                        week = lubridate::week(date),
                        wday = lubridate::wday(date),
                        calls100 = calls/100,
                        call_per_sick = calls/n_sick, 
                        day = lubridate::day(date))
plot(sick[,c(4:8)])
plot(sick[,c(2,5:8)])
plot(sick[,c(3,5:8)])
plot(sick[,c(10,4:8)])
plot(sick[,c(2,7,9)])
```

Regarding the figures all three variables in question are mostly affected by the month and the week features. Plus, there seems to be a slight dependency of the target variable sby_need to the new calls_per_sick feature (the value range is similar, so we'll get them on one plot). Let's get into more detail on this.

```{r}
#| message: false
#| warning: false
#| label: reshape and aggregate
require(ggplot2, quietly = T)
require(plotly, quietly = T)
require(data.table, quietly = T)
require(reshape2, quietly = T)

# reshape tables
week <- reshape2::melt(sick[,c(2, 4,7,9,10)], id.vars = "week") %>% 
  dplyr::group_by(week, variable) %>%
  dplyr::summarise(n = mean(value))

month <- reshape2::melt(sick[,c(2,4,6,9,10)], id.vars = "month") %>% 
  dplyr::group_by(month, variable) %>%
  dplyr::summarise(n = mean(value))

weekday <- reshape2::melt(sick[,c(2,4,8,10)], id.vars = "wday") %>% 
  dplyr::group_by(wday, variable) %>%
  dplyr::summarise(n = mean(value))

p_weekday <- ggplot(data = weekday) +
  geom_line(aes(x = wday, y = n, colour = variable)) +
  geom_smooth(aes(x = wday, y = n, colour = variable))

p_week <- ggplot(data = week) +
  geom_line(aes(x = week, y = n, colour = variable)) +
  geom_smooth(aes(x = week, y = n, colour = variable))

p_month <- ggplot(data = month) +
  geom_line(aes(x = month, y = n, colour = variable)) +
  geom_smooth(aes(x = month, y = n, colour = variable))


ggplotly(p_week)
ggplotly(p_month)

```

It looks like, with call_per_sick, we found a promising predictor of the target variable, with almost parallel smooth curves. For the seasonality the week scale seems fine, the monthly analyse was rather a little to general. Interestingly in the week scale, one can see periodically reoccurring peaks every 3-5 weeks between week 5 and 44. For call_per_sick the peaks are not as clearly visible as for sby_need and do sometimes have an offset to the latter. On the monthly scale, those peaks disappear completely, but we do see some higher values especially in the months may and august.

A 3-5 weeks period looks like the day of month could be of relevance. The next plot compares therefore the day of month to the possible predictors.

```{r}
#| message: false
#| warning: false
day <- reshape2::melt(sick[,c(2,4,9,10,11)], id.vars = "day") %>% 
  dplyr::group_by(day, variable) %>%
  dplyr::summarise(n = mean(value))

p_day <- ggplot(data = day) +
  geom_line(aes(x = day, y = n, colour = variable)) +
  geom_smooth(aes(x = day, y = n, colour = variable))

ggplotly(p_day)
```

The smooth curve shows an obvious downward trend at the beginning of each month. Smaller peaks occur approximately every three days. Regarding a seasonality pattern, the day of month is surly to be kept in mind.

Let's also check the weekday scale, just to be complete.

```{r}
#| message: false
#| warning: false
ggplotly(p_weekday)


```

According to the smooth curve there seems to be a strong variance in the weekday pattern. Appearently, the lines contain no additional information.

Lastly, see what it looks like if we plot the continuous weekly scale over the whole period. Possibly, there is a larger trend as well.

```{r}
#| message: false
#| warning: false
weeks_per_year <- reshape2::melt(sick[,c(4,5,7,10)],
                                 id.vars = c("year","week")) %>%
  dplyr::group_by(year, week, variable) %>%
  dplyr::summarise(n = mean(value)) %>%
  dplyr::mutate(contweek = year + week/52)
  
  

p_wpy <- ggplot(data = weeks_per_year) +
  geom_line(aes(x = contweek, 
                # x = week,
                y = n,
                colour = variable)) +
                # colour = as.factor(paste0(variable,year)), 
                # group = year)) +
  geom_smooth(aes(x = contweek, 
                  # x = week,
                  y = n, 
                  colour = variable))
                  # colour = as.factor(paste0(variable,year)), 
                  # group = year))

ggplotly(p_wpy)

```

The variable call_per_sick slightly decreases in the long trend but still seems to be a good predictor until start of 2019. By 2019 call_per_sick loses its predictive ability due to the rapid increase of sby_need. There are two ideas to possibly face that.

1.  Add call100 to this figure to see if this feature could help
2.  Try logarithmic scale, if we could flatten the sby_need trend, adding 1 to where sby_need == 0 for displaying logarithmic values.

```{r}
#| message: false
#| warning: false
weeks_per_year2 <- reshape2::melt(sick[,c(4,5,7,9,10)],
                                 id.vars = c("year","week")) %>%
  dplyr::group_by(year, week, variable) %>%
  dplyr::summarise(n = mean(value)) %>%
  dplyr::mutate(contweek = year + week/52)
  
# add 10 to sby_need == 0 to avoid log(0)
weeks_per_year2[(weeks_per_year2$variable == "sby_need" & 
                 weeks_per_year2$n == 0), "n"] <- 1

p_wpy2 <- ggplot(data = weeks_per_year2) +
  geom_line(aes(x = contweek, 
                # x = week,
                # y = n,
                y = log(n),
                colour = variable)) +
                # colour = as.factor(paste0(variable,year)), 
                # group = year)) +
  geom_smooth(aes(x = contweek, 
                  # x = week,
                  # y = n, 
                  y = log(n), 
                  colour = variable))
                  # colour = as.factor(paste0(variable,year)), 
                  # group = year))

ggplotly(p_wpy2)
```

Conclusion:

1.  calls100 doesn't help.
2.  We do have a positive trend over the years with a sharp increase in the need of standby personnel since start of 2019
3.  The logarithmic scale seems to have a positive effect on the ability for call_per_sick to work as a predictor regarding the smooth curve. We could try to acknowledge that if we divide log(call_per_sick) by log(sby_need). The more flatten the line, the more dependency we have (see figure below).
4.  A linear regression based on smooth curves will probably not work out well. We still need to consider a strong weekly seasonality and a minor monthly seasonality.

```{r}
#| message: false
#| warning: false
cps <- weeks_per_year2[weeks_per_year2$variable == "call_per_sick",]$n
need <- weeks_per_year2[weeks_per_year2$variable == "sby_need",]$n

p <- ggplot(data.frame(date = weeks_per_year2$contweek,
                       dep_check = log(cps)/log(need)),
            aes(x = date,
                y = dep_check)) +
  geom_line() + 
  geom_smooth()

ggplotly(p)
```

That doesn't look too bad. We do have reoccurring patterns and we have a small, almost linear trend on the smooth curve. Let's work with that too.

## Werteverteilung der Zielvariable

Die Verteilung der Zielvariablen auf Tagesbasis, also nicht in aggregierter Form, soll schlussendlich noch Auschluss, bzw. einen Überblick über die zu erwartenden Werte liefern.

```{r}
hist <- ggplot(data = sickness_csv, 
               aes(x = sby_need, fill = as.factor(year(date)))) +
  geom_histogram(binwidth = 10, position = "dodge") 
ggplotly(hist)

```

erkannt: Masse bei den Werte 0-10

kleine Gruppe an Werten von 418 bis 456 aus jedem Jahr =\> selber Tag? jährliche Veranstaltung?

```{r}
sickness_csv[between(sickness_csv$sby_need,418,456),]$date
```

nicht bewahrheitet

? Quantillen als mögliche Anhaltepunkte für einen fixen "Sicherheitspolster" in der Vorhersage

```{r}



dens <- ggplot(data = sickness_csv, 
               aes(x = sby_need, fill = as.factor(year(date)))) +
  geom_density()
ggplotly(dens)

summary(sickness_csv$sby_need)

sum(table(sickness_csv[sickness_csv$sby_need <= 10,]$sby_need))/1152


quantile(sickness_csv$sby_need, probs = c(0.95, 0.90, 0.85, 0.8, 0.68))
```

Quantillen 68% und 75% 80% und 85%, wenige aber starke Ausreißer =\> logarithmische Skalierung um die Werte besser zu verteilen.

```{r}

sickness_csv$logscale <- log(sickness_csv$sby_need + 1)

dens <- ggplot(data = sickness_csv, 
               aes(x = logscale, fill = as.factor(year(date)))) +
  geom_density()
ggplotly(dens)

summary(sickness_csv$logscale)

quantile(sickness_csv$logscale, probs = c(0.95, 0.90, 0.85, 0.8, 0.75, 0.68))


```

besser erkennbar in Grafik und "schöner" verteilt. Beschreibung der Grafik

die Jahre 2016 und 2017 sind nahezu gleich, Änderungen vor allem in 2018 und im ersten Drittel 2019. Dennoch in allen Jahren eine mehr oder weniger ausgprägte "zweite Erhebung" zw. den Werten 4 und 6 erkennbar.

Selbst wenn 50 als Fixwert mit Sicherheitsreserve (statt 0-Vorhersagen) gewählt wird, können täglich immer noch (90 - 50) \* 165 Euro (€6.600,-), nur anhand der explorativen Datenanalyse, eingespart werden und man hätte mit 80% Wahrscheinlichkeit immer noch genug StandBy-Personal. Wenn der Vorhersage-Algorithmus die restlichen 20% Ausreißer "findet", ist das Ziel erreicht.

Algorithmen werden vermutlich mit log-Werten besser umgehen können, weil sich Ausreißer weniger Stark auswirken.

### Zeitreihenanalyse

vgl. [@hyndman_forecasting_2021]

```{r}
#| label: load packages
#| message: false
#| warning: false
#| include: false

require(fable)
require(tsibble)
require(lubridate)
require(dplyr)
require(tidyverse)
require(readr)
require(plotly)
require(fpp3)
require(ggplot2)
```

#### create tsibble

```{r}
sickness_csv$date <- as.Date(sickness_csv$date)
ts_sick <- sickness_csv %>% 
  select(date, sby_need) %>%
  as_tsibble(index = date)
```

#### Plot sby_need from tsibble

```{r}
ts_sick %>%
  autoplot() %>%
  labs(title = "Einberufenes StandBy Personal",
       subtitle = "Gesamtübersicht",
       y = "Anzahl Personal")
```

#### Seasonal Plot

```{r}
ts_sick %>%
  gg_season(sby_need, period = "1y") %>%
  labs(title = "Einberufenes StandBy Personal",
       subtitle = "Saisonale Übersicht",
       y = "Anzahl Personal")


```

keine eindeutige saisonalität erkennbar, vielleicht im aggregierten wochendatensatz ähnlich @sec-focus-on-specific-variables

Warum summarise to max(value)

```{r}
#| message: false
#| warning: false
#| paged-print: false
ts_week <- sickness_csv %>%
  mutate(week = yearweek(date),
         calls100 = calls/100,
         calls_per_sick = calls / n_sick) %>%
  select(week, n_sick, calls100, calls_per_sick, sby_need) %>%
  reshape2::melt(id.vars = "week") %>%
  group_by(week, variable) %>%
  summarise(max_value = max(value)) %>%
  as_tsibble(key = variable,
             index = week)
```

```{r}
ts_week %>% 
  autoplot()
```

```{r}
ts_week %>%
  gg_season(max_value, period = "1y")
```

#### ACF

```{r}
# Tagesdaten
ts_sick %>% 
  ACF(sby_need, lag_max = 60) %>%
  autoplot()
```

Die höhere Autokorrelation in den niedriegen lags zeigt, dass die Daten einem Trend nterliegen. Die kleinen Peaks bei 6, 13, und 22 zeigen eine leichte wochenweise Saisonalität und die höheren Ausschläge bei 28, 31 und 33 entsprechen der untersuchten Saisonalität von vier bis fünf Wochen. Die bisherigen Annahmen werden bestätigt.

```{r}
# Wochendaten
ts_week %>% 
  ACF(max_value, lag_max = 60) %>%
  autoplot()


```

shannon entropy: Aussage über Leichtigkeit der Vorhersage

```{r}
ts_sick %>% features(sby_need, feat_spectral)

ts_week %>% features(log_value, feat_spectral)

```

Interpretation Autokorrelationsplots und -Werte!!

```{r}
ts_week %>% features(max_value, feat_acf)
```

#### Decomposition

```{r}
ts_sick %>%
  model(
    STL(sby_need ~ trend(window = 7) +
                   season(window = "periodic"),
    robust = TRUE)) %>%
  components() %>%
  autoplot()
```

```{r}
ts_week %>%
  model(
    STL(max_value ~ trend(window = 52) +
                   season(window = "periodic"),
    robust = TRUE)) %>%
  components() %>%
  autoplot()
```

```{r}
ts_week$log_value <- log(ts_week$max_value + 1)
ts_week %>%
  model(
    STL(log_value ~ trend(window = 53) +
                   season(period = "1 year"),
    robust = TRUE)) %>%
  components() %>%
  autoplot()

```

Ziel: negativer remainder um zu niedrige Schätzungen zu durch Saisonales Modell zu vermeiden.

Höhere Volatilität in remainder ab Ende 2018 erkennbar.

Trend kann als Basiswert für Standbypersonal herangezogen werden

```{r}
mq <- function(x, q = 0.9, frame = 7) {
  y <- numeric()
  for (i in 1:length(x)) {
    base::ifelse(
      ((i - floor(frame/2)) <= 0) |
        ((i + floor(frame/2)) >= length(x)),
           value <- 0,
           value <- quantile(x[(i - floor(frame/2)):(i + floor(frame/2))],
                             q))
    y <- c(y, value)
  }
  return(y)
}
```

```{r}
ts_sick_mq <- ts_sick %>%
  mutate(mq = mq(sby_need, q = 0.99, frame = 8),
         mq2 = mq(mq, q = 0.99, frame = 8),
         ma = ma(mq, order = 8, centre = TRUE)
         )

ts_sick_mq %>%
  autoplot(mq2)

```

```{r}
ts_sick_mq %>% features(mq, feat_spectral)
ts_sick_mq %>% features(mq2, feat_spectral)
ts_sick_mq %>% features(ma, feat_spectral)
```

```{r}
ts_week_mq <- ts_week %>%
  mutate(
    mq = mq(log_value, q = 0.99, frame = 5),
    mq2 = mq(mq, q = 0.99, frame = 3),
    ma = ma(mq, order = 5, centre = TRUE)
    )

ts_week_mq %>%
  autoplot(mq2)

```

```{r}
ts_week_mq %>% features(mq2, feat_spectral)
```

```{r}
ts_sick_mq %>%
  model(
    STL(mq2 ~ trend(window = 53) +
          season(period = "1 year"),
        robust = TRUE)) %>%
  components() %>%
  autoplot()
```

## Over-all Conclusion:

1.  We do have a positive trend over the years with a sharp increase in the need of standby personnel since start of 2019
2.  We possibly can use the call_per_sick feature as predictor regarding the logarithmic smooth curve on a weekly scale. Still, in this case the calls and the number of sick drivers needs to be predicted first, because they will not be known in advance. According to the day of month-plot, that might a doable task.
3.  A linear regression based on smooth curves alone will probably not work out well. We still need to consider a strong weekly seasonality, a dependence on the day of month and a minor monthly seasonality. To properly forceast sby_need values, a decomposition of the time-series data could be advisable.

# Literaturverzeichnis
